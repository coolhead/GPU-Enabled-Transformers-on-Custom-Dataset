{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef1aa26-12f4-46cb-b178-b164fd88c036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_sarcastic': 1, 'headline': 'thirtysomething scientists unveil doomsday clock of hair loss', 'article_link': 'https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load local sarcasm dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"C:/Users/RAGHU/datasets/Sarcasm_Headlines_Dataset_v2.json\", split=\"train\")\n",
    "\n",
    "# Check format\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a27c0e-645d-45c6-a390-47ed619d5016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 12:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.252157</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.885204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.254197</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.900763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.246000</td>\n",
       "      <td>0.441977</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.896104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation: {'eval_loss': 0.25215739011764526, 'eval_accuracy': 0.8875, 'eval_f1': 0.8852040816326531, 'eval_runtime': 1.3416, 'eval_samples_per_second': 596.306, 'eval_steps_per_second': 18.635, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Using GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "   CUDA Version: 12.1\n",
      "   cuDNN Enabled: True\n",
      "\n",
      "üîç I'm thrilled to be ignored for the third time today.\n",
      " ‚Üí Prediction: Not Sarcastic (93.44%)\n",
      "\n",
      "üîç It's raining again. What a surprise.\n",
      " ‚Üí Prediction: Not Sarcastic (95.88%)\n",
      "\n",
      "üîç This software never crashes! Oh wait...\n",
      " ‚Üí Prediction: Not Sarcastic (89.01%)\n",
      "\n",
      "üîç Thank you so much for your generous fine!\n",
      " ‚Üí Prediction: Not Sarcastic (96.26%)\n",
      "\n",
      "üîç The weather is genuinely beautiful today.\n",
      " ‚Üí Prediction: Not Sarcastic (98.68%)\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Optimized Sarcasm Classifier with Accelerated Transformers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, ClassLabel, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ 1. Load and Balance Dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"C:/Users/RAGHU/datasets/Sarcasm_Headlines_Dataset_v2.json\", split=\"train\").shuffle(seed=42)\n",
    "sarc = dataset.filter(lambda x: x[\"is_sarcastic\"] == 1).select(range(4000))\n",
    "non_sarc = dataset.filter(lambda x: x[\"is_sarcastic\"] == 0).select(range(4000))\n",
    "balanced_dataset = concatenate_datasets([sarc, non_sarc]).shuffle(seed=42)\n",
    "label_features = ClassLabel(names=[\"not_sarcastic\", \"sarcastic\"])\n",
    "balanced_dataset = balanced_dataset.cast_column(\"is_sarcastic\", label_features)\n",
    "\n",
    "# ‚úÖ 2. Tokenization\n",
    "model_checkpoint = \"distilroberta-base\"  # smaller & faster than bert-base-uncased\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = example[\"is_sarcastic\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = balanced_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ‚úÖ 3. Split\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = split[\"train\"]\n",
    "eval_ds = split[\"test\"]\n",
    "\n",
    "# ‚úÖ 4. Model & Data Collator\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(\"cuda\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ‚úÖ 5. Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# ‚úÖ 6. Training Args (With GPU Optimizations)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ‚úÖ 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ‚úÖ 8. Train and Evaluate\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"\\nüìä Evaluation:\", results)\n",
    "\n",
    "# ‚úÖ 9. Save\n",
    "model_path = \"./fine-tuned-sarcasm-distilroberta\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# ‚úÖ 10. Inference Pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# ‚úÖ 11. Device Info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n‚úÖ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"   CUDA Version:\", torch.version.cuda)\n",
    "    print(\"   cuDNN Enabled:\", torch.backends.cudnn.enabled)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Running on CPU.\")\n",
    "\n",
    "# ‚úÖ 12. Human-Readable Output\n",
    "label_map = {0: \"Not Sarcastic\", 1: \"Sarcastic\"}\n",
    "examples = [\n",
    "    \"I'm thrilled to be ignored for the third time today.\",\n",
    "    \"It's raining again. What a surprise.\",\n",
    "    \"This software never crashes! Oh wait...\",\n",
    "    \"Thank you so much for your generous fine!\",\n",
    "    \"The weather is genuinely beautiful today.\"\n",
    "]\n",
    "\n",
    "for sentence in examples:\n",
    "    output = classifier(sentence)[0]\n",
    "    label_id = int(output[\"label\"].split(\"_\")[-1])\n",
    "    print(f\"\\nüîç {sentence}\\n ‚Üí Prediction: {label_map[label_id]} ({output['score']:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103f697c-ebc6-4f40-87f5-5064b7f8635b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Evaluation Results:\n",
      "                                                Input Text          Expected     Predicted Confidence Correct\n",
      "I'm so glad my phone died right when I needed it the most.     Sarcastic (1) Not Sarcastic     99.97%       ‚ùå\n",
      "                   Oh joy, another Monday morning meeting!     Sarcastic (1) Not Sarcastic     99.95%       ‚ùå\n",
      "             Perfect! My favorite show got canceled again!     Sarcastic (1) Not Sarcastic     99.91%       ‚ùå\n",
      "              I'm absolutely thrilled to do your work too.     Sarcastic (1) Not Sarcastic     99.98%       ‚ùå\n",
      "             What a brilliant update, it broke everything!     Sarcastic (1) Not Sarcastic     99.97%       ‚ùå\n",
      "                    I really enjoyed the movie last night. Not Sarcastic (0) Not Sarcastic     99.97%       ‚úÖ\n",
      "                           The weather is beautiful today. Not Sarcastic (0) Not Sarcastic     99.98%       ‚úÖ\n",
      "                      Thanks for your help on the project! Not Sarcastic (0) Not Sarcastic     99.96%       ‚úÖ\n",
      "                           This book was very informative. Not Sarcastic (0) Not Sarcastic     99.97%       ‚úÖ\n",
      "            I finally completed my marathon training goal. Not Sarcastic (0) Not Sarcastic     99.97%       ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Evaluation Script for Sarcasm Classifier\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Human-labeled sarcasm test examples\n",
    "examples = [\n",
    "    {\"text\": \"I'm so glad my phone died right when I needed it the most.\", \"label\": 1},\n",
    "    {\"text\": \"Oh joy, another Monday morning meeting!\", \"label\": 1},\n",
    "    {\"text\": \"Perfect! My favorite show got canceled again!\", \"label\": 1},\n",
    "    {\"text\": \"I'm absolutely thrilled to do your work too.\", \"label\": 1},\n",
    "    {\"text\": \"What a brilliant update, it broke everything!\", \"label\": 1},\n",
    "    {\"text\": \"I really enjoyed the movie last night.\", \"label\": 0},\n",
    "    {\"text\": \"The weather is beautiful today.\", \"label\": 0},\n",
    "    {\"text\": \"Thanks for your help on the project!\", \"label\": 0},\n",
    "    {\"text\": \"This book was very informative.\", \"label\": 0},\n",
    "    {\"text\": \"I finally completed my marathon training goal.\", \"label\": 0},\n",
    "]\n",
    "\n",
    "# ‚úÖ Load the fine-tuned model\n",
    "model_path = \"./fine-tuned-sarcasm-detector\"\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "\n",
    "# ‚úÖ Label decoding\n",
    "label_map = {\"LABEL_0\": \"Not Sarcastic\", \"LABEL_1\": \"Sarcastic\"}\n",
    "\n",
    "# ‚úÖ Prediction and comparison\n",
    "results = []\n",
    "for item in examples:\n",
    "    prediction = classifier(item[\"text\"])[0]\n",
    "    predicted_label = 1 if prediction[\"label\"] == \"LABEL_1\" else 0\n",
    "    results.append({\n",
    "        \"Input Text\": item[\"text\"],\n",
    "        \"Expected\": \"Sarcastic (1)\" if item[\"label\"] == 1 else \"Not Sarcastic (0)\",\n",
    "        \"Predicted\": label_map[prediction[\"label\"]],\n",
    "        \"Confidence\": f\"{prediction['score']:.2%}\",\n",
    "        \"Correct\": \"‚úÖ\" if predicted_label == item[\"label\"] else \"‚ùå\"\n",
    "    })\n",
    "\n",
    "# ‚úÖ Display as DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nüéØ Evaluation Results:\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9f2d93-ca2f-4fd4-9443-69dc3cd4bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 01:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.276545</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.876923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.251266</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.903704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.373197</td>\n",
       "      <td>0.903750</td>\n",
       "      <td>0.905055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation: {'eval_loss': 0.2512662410736084, 'eval_accuracy': 0.9025, 'eval_f1': 0.9037037037037037, 'eval_runtime': 1.678, 'eval_samples_per_second': 476.764, 'eval_steps_per_second': 14.899, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Using GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "   CUDA Version: 12.1\n",
      "   cuDNN Enabled: True\n",
      "\n",
      "üéØ Evaluation Results:\n",
      "‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï\n",
      "‚îÇ Input Text                                                 ‚îÇ Expected          ‚îÇ Predicted     ‚îÇ Confidence   ‚îÇ Correct   ‚îÇ\n",
      "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
      "‚îÇ I'm so glad my phone died right when I needed it the most. ‚îÇ Sarcastic (1)     ‚îÇ Not Sarcastic ‚îÇ 98.50%       ‚îÇ ‚ùå        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Oh joy, another Monday morning meeting!                    ‚îÇ Sarcastic (1)     ‚îÇ Not Sarcastic ‚îÇ 89.98%       ‚îÇ ‚ùå        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Perfect! My favorite show got canceled again!              ‚îÇ Sarcastic (1)     ‚îÇ Not Sarcastic ‚îÇ 96.73%       ‚îÇ ‚ùå        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ I'm absolutely thrilled to do your work too.               ‚îÇ Sarcastic (1)     ‚îÇ Not Sarcastic ‚îÇ 98.05%       ‚îÇ ‚ùå        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ What a brilliant update, it broke everything!              ‚îÇ Sarcastic (1)     ‚îÇ Not Sarcastic ‚îÇ 98.62%       ‚îÇ ‚ùå        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ I really enjoyed the movie last night.                     ‚îÇ Not Sarcastic (0) ‚îÇ Not Sarcastic ‚îÇ 96.50%       ‚îÇ ‚úÖ        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ The weather is beautiful today.                            ‚îÇ Not Sarcastic (0) ‚îÇ Not Sarcastic ‚îÇ 99.62%       ‚îÇ ‚úÖ        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ Thanks for your help on the project!                       ‚îÇ Not Sarcastic (0) ‚îÇ Not Sarcastic ‚îÇ 99.02%       ‚îÇ ‚úÖ        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ This book was very informative.                            ‚îÇ Not Sarcastic (0) ‚îÇ Not Sarcastic ‚îÇ 97.73%       ‚îÇ ‚úÖ        ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ I finally completed my marathon training goal.             ‚îÇ Not Sarcastic (0) ‚îÇ Not Sarcastic ‚îÇ 98.63%       ‚îÇ ‚úÖ        ‚îÇ\n",
      "‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Optimized Sarcasm Classifier with Human-Evaluated Inference\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, ClassLabel, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ‚úÖ 1. Load and Balance Dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"C:/Users/RAGHU/datasets/Sarcasm_Headlines_Dataset_v2.json\", split=\"train\").shuffle(seed=42)\n",
    "sarc = dataset.filter(lambda x: x[\"is_sarcastic\"] == 1).select(range(4000))\n",
    "non_sarc = dataset.filter(lambda x: x[\"is_sarcastic\"] == 0).select(range(4000))\n",
    "balanced_dataset = concatenate_datasets([sarc, non_sarc]).shuffle(seed=42)\n",
    "label_features = ClassLabel(names=[\"not_sarcastic\", \"sarcastic\"])\n",
    "balanced_dataset = balanced_dataset.cast_column(\"is_sarcastic\", label_features)\n",
    "\n",
    "# ‚úÖ 2. Tokenization\n",
    "model_checkpoint = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = example[\"is_sarcastic\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = balanced_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ‚úÖ 3. Split\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = split[\"train\"]\n",
    "eval_ds = split[\"test\"]\n",
    "\n",
    "# ‚úÖ 4. Model & Data Collator\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(\"cuda\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ‚úÖ 5. Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# ‚úÖ 6. Training Args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ‚úÖ 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ‚úÖ 8. Train and Evaluate\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(\"\\nüìä Evaluation:\", results)\n",
    "\n",
    "# ‚úÖ 9. Save\n",
    "model_path = \"./fine-tuned-sarcasm-distilroberta\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# ‚úÖ 10. Inference Pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# ‚úÖ 11. Device Info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n‚úÖ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"   CUDA Version:\", torch.version.cuda)\n",
    "    print(\"   cuDNN Enabled:\", torch.backends.cudnn.enabled)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Running on CPU.\")\n",
    "\n",
    "# ‚úÖ 12. Human-Evaluated Test Set\n",
    "human_test_examples = [\n",
    "    (\"I'm so glad my phone died right when I needed it the most.\", 1),\n",
    "    (\"Oh joy, another Monday morning meeting!\", 1),\n",
    "    (\"Perfect! My favorite show got canceled again!\", 1),\n",
    "    (\"I'm absolutely thrilled to do your work too.\", 1),\n",
    "    (\"What a brilliant update, it broke everything!\", 1),\n",
    "    (\"I really enjoyed the movie last night.\", 0),\n",
    "    (\"The weather is beautiful today.\", 0),\n",
    "    (\"Thanks for your help on the project!\", 0),\n",
    "    (\"This book was very informative.\", 0),\n",
    "    (\"I finally completed my marathon training goal.\", 0)\n",
    "]\n",
    "\n",
    "# ‚úÖ 13. Evaluate on Human Test Set\n",
    "results_table = []\n",
    "label_map = {0: \"Not Sarcastic\", 1: \"Sarcastic\"}\n",
    "\n",
    "for text, expected in human_test_examples:\n",
    "    output = classifier(text)[0]\n",
    "    label_id = int(output[\"label\"].split(\"_\")[-1])\n",
    "    prediction = label_map[label_id]\n",
    "    expected_str = label_map[expected]\n",
    "    is_correct = label_id == expected\n",
    "    emoji = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "    results_table.append([text, f\"{expected_str} ({expected})\", prediction, f\"{output['score']:.2%}\", emoji])\n",
    "\n",
    "# ‚úÖ 14. Display\n",
    "print(\"\\nüéØ Evaluation Results:\")\n",
    "print(tabulate(results_table, headers=[\"Input Text\", \"Expected\", \"Predicted\", \"Confidence\", \"Correct\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8bb32e6-a173-430f-8c24-d8262bbbbcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e02c75eb2c41518e6652390fa05a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAGHU\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RAGHU\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386c75825bb1456e976f9b50cca6dec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56d753471594656b8dafcb84e6fb417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a918ae633375481983998ebb946d21c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698f73479d2c4f738fea66ac90cdf431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç I'm so glad my phone died right when I needed it the most.\n",
      " ‚Üí Prediction: Sarcastic (56.53%)\n",
      "\n",
      "üîç Oh great, another email from the boss at 2 AM.\n",
      " ‚Üí Prediction: Not sarcastic (57.62%)\n",
      "\n",
      "üîç I really enjoyed the movie last night.\n",
      " ‚Üí Prediction: Not sarcastic (69.73%)\n",
      "\n",
      "üîç Thank you so much for your generous fine.\n",
      " ‚Üí Prediction: Not sarcastic (68.02%)\n",
      "\n",
      "üîç The weather is beautiful today.\n",
      " ‚Üí Prediction: Not sarcastic (71.83%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the zero-shot classifier\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Input examples\n",
    "examples = [\n",
    "    \"I'm so glad my phone died right when I needed it the most.\",\n",
    "    \"Oh great, another email from the boss at 2 AM.\",\n",
    "    \"I really enjoyed the movie last night.\",\n",
    "    \"Thank you so much for your generous fine.\",\n",
    "    \"The weather is beautiful today.\"\n",
    "]\n",
    "\n",
    "# Define candidate labels\n",
    "labels = [\"sarcastic\", \"not sarcastic\"]\n",
    "\n",
    "# Run classification\n",
    "for sentence in examples:\n",
    "    result = classifier(sentence, labels)\n",
    "    prediction = result[\"labels\"][0]\n",
    "    score = result[\"scores\"][0]\n",
    "    print(f\"\\nüîç {sentence}\")\n",
    "    print(f\" ‚Üí Prediction: {prediction.capitalize()} ({score:.2%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6e744-4be4-4a2b-b9d5-dfaedec4d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Sarcasm Classifier Fine-Tuning with GPU using Hugging Face (Enhanced Version)\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import load_dataset, ClassLabel, DatasetDict\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ 1. Load and Balance the Sarcasm Dataset\n",
    "path = \"C:/Users/RAGHU/datasets/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "dataset = load_dataset(\"json\", data_files=path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Balance the dataset (equal sarcastic and non-sarcastic samples)\n",
    "sarc = dataset.filter(lambda x: x[\"is_sarcastic\"] == 1).select(range(7000))\n",
    "non_sarc = dataset.filter(lambda x: x[\"is_sarcastic\"] == 0).select(range(7000))\n",
    "from datasets import concatenate_datasets\n",
    "balanced_dataset = concatenate_datasets([sarc, non_sarc]).shuffle(seed=42)\n",
    "\n",
    "\n",
    "# ‚úÖ 2. Convert labels to ClassLabel\n",
    "label_features = ClassLabel(names=[\"not_sarcastic\", \"sarcastic\"])\n",
    "balanced_dataset = balanced_dataset.cast_column(\"is_sarcastic\", label_features)\n",
    "\n",
    "# ‚úÖ 3. Tokenization\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = example[\"is_sarcastic\"]\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = balanced_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ‚úÖ 4. Split the Dataset\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_ds = split[\"train\"]\n",
    "test_ds = split[\"test\"]\n",
    "\n",
    "# ‚úÖ 5. Model Setup\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(\"cuda\")\n",
    "\n",
    "# ‚úÖ 6. Data Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ‚úÖ 7. Metric Function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# ‚úÖ 8. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ‚úÖ 9. Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ‚úÖ 10. Train\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ 11. Save\n",
    "model_path = \"./fine-tuned-sarcasm-detector\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# ‚úÖ 12. Inference Pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# ‚úÖ 13. Hardware Info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n‚úÖ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"   CUDA Version:\", torch.version.cuda)\n",
    "    print(\"   cuDNN Enabled:\", torch.backends.cudnn.enabled)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Running on CPU. Consider enabling a GPU for better performance.\")\n",
    "\n",
    "# ‚úÖ 14. Label Mapping and Inference\n",
    "label_map = {0: \"Not Sarcastic\", 1: \"Sarcastic\"}\n",
    "examples = [\n",
    "    \"The meeting was incredibly productive. Everyone just yelled for two hours.\",\n",
    "    \"I absolutely love it when my computer crashes.\",\n",
    "    \"The weather is lovely today and I‚Äôm having a great time.\",\n",
    "    \"Oh fantastic, another email from my boss at midnight!\",\n",
    "    \"Thanks a lot for your help... not.\"\n",
    "]\n",
    "\n",
    "for sentence in examples:\n",
    "    output = classifier(sentence)[0]\n",
    "    label_id = int(output[\"label\"].split(\"_\")[-1])\n",
    "    print(f\"\\nüîç {sentence}\\n ‚Üí Prediction: {label_map[label_id]} ({output['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b6745-46d0-4100-be64-0ae099febbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Fine-Tune a Sarcasm Detector with Hugging Face + GPU + Improvements\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ‚úÖ 1. Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "# ‚úÖ 2. Load and Balance Dataset (local JSON file)\n",
    "dataset = load_dataset(\"json\", data_files=\"C:/Users/RAGHU/datasets/Sarcasm_Headlines_Dataset_v2.json\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# ‚úÖ 3. Preprocess (use only headline and is_sarcastic)\n",
    "dataset = dataset.remove_columns([\"article_link\"])\n",
    "\n",
    "# ‚úÖ 4. Tokenizer Setup\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"headline\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "tokenized = tokenized.rename_column(\"is_sarcastic\", \"labels\")\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# ‚úÖ 5. Train/Validation Split\n",
    "split = tokenized.train_test_split(test_size=0.1)\n",
    "train_ds = split[\"train\"]\n",
    "eval_ds = split[\"test\"]\n",
    "\n",
    "# ‚úÖ 6. Model Initialization\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2).to(\"cuda\")\n",
    "\n",
    "# ‚úÖ 7. Data Collator\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ‚úÖ 8. Metric Computation\n",
    "def compute_metrics(pred):\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(pred.label_ids, preds),\n",
    "        \"f1\": f1_score(pred.label_ids, preds)\n",
    "    }\n",
    "\n",
    "# ‚úÖ 9. Training Arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./sarcasm_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ‚úÖ 10. Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ‚úÖ 11. Train\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ 12. Save\n",
    "trainer.save_model(\"./fine-tuned-sarcasm\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-sarcasm\")\n",
    "\n",
    "# ‚úÖ 13. Inference - Human-Friendly Output\n",
    "\n",
    "# Show device info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"   CUDA Version :\", torch.version.cuda)\n",
    "    print(\"   cuDNN Enabled:\", torch.backends.cudnn.enabled)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Running on CPU. Consider enabling GPU acceleration for faster training/inference.\")\n",
    "\n",
    "# Map model output labels to readable form\n",
    "label_map = {\"LABEL_0\": \"Not Sarcastic\", \"LABEL_1\": \"Sarcastic\"}\n",
    "\n",
    "# Examples to classify\n",
    "examples = [\n",
    "    \"Oh great, another Monday!\",\n",
    "    \"I just love getting stuck in traffic for hours.\",\n",
    "    \"Thank you for the gift, it's exactly what I never wanted.\",\n",
    "    \"The weather is lovely today and I‚Äôm having a great time.\"\n",
    "]\n",
    "\n",
    "print(\"\\nüéØ Inference Results:\")\n",
    "for sentence in examples:\n",
    "    prediction = classifier(sentence)[0]\n",
    "    label = label_map.get(prediction[\"label\"], prediction[\"label\"])\n",
    "    score = prediction[\"score\"]\n",
    "    print(f\"üó£ \\\"{sentence}\\\"\\n ‚Üí Prediction: {label} ({score:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c40f75-d6e9-46ba-84f7-e9a2cf68475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Sarcasm Classifier Fine-Tuning with GPU using Hugging Face\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline,\n",
    ")\n",
    "from datasets import load_dataset, ClassLabel\n",
    "import torch\n",
    "\n",
    "# ‚úÖ 1. Load the Sarcasm Dataset\n",
    "path = \"C:/Users/RAGHU/datasets/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "dataset = load_dataset(\"json\", data_files=path, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# ‚úÖ 2. Convert labels to ClassLabel if needed\n",
    "label_features = ClassLabel(names=[\"not_sarcastic\", \"sarcastic\"])\n",
    "dataset = dataset.cast_column(\"is_sarcastic\", label_features)\n",
    "\n",
    "# ‚úÖ 3. Tokenizer & Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokens = tokenizer(example[\"headline\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    tokens[\"labels\"] = example[\"is_sarcastic\"]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ‚úÖ 4. Model Setup\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2).to(\"cuda\")\n",
    "\n",
    "# ‚úÖ 5. Data Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ‚úÖ 6. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# ‚úÖ 7. Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset.select(range(8000)),  # use a subset for quicker training\n",
    "#    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ‚úÖ 8. Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ 9. Save the Model\n",
    "model_path = \"./fine-tuned-sarcasm-detector\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# ‚úÖ 10. Inference Pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# ‚úÖ Test Examples\n",
    "#print(\"\\nüü¢ Honest:\", classifier(\"The government passed a great policy today!\"))\n",
    "#print(\"\\nüî¥ Sarcasm:\", classifier(\"Oh wow, what a brilliant idea! Let‚Äôs ignore science again.\"))\n",
    "\n",
    "# Better device info\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"   CUDA Version :\", torch.version.cuda)\n",
    "    print(\"   cuDNN Enabled:\", torch.backends.cudnn.enabled)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Running on CPU. Consider enabling a GPU for better performance.\")\n",
    "\n",
    "# Human-readable labels\n",
    "label_map = {0: \"Not Sarcastic\", 1: \"Sarcastic\"}\n",
    "\n",
    "examples = [\n",
    "    \"The government passed a great policy today!\",\n",
    "    \"Oh wow, what a brilliant idea! Let‚Äôs ignore science again.\",\n",
    "    \"Totally what I wanted to hear at 3 a.m.\",\n",
    "    \"I'm just thrilled the meeting was extended by 2 hours.\"\n",
    "]\n",
    "\n",
    "for sentence in examples:\n",
    "    output = classifier(sentence)[0]\n",
    "    label_id = int(output[\"label\"].split(\"_\")[-1])\n",
    "    print(f\"\\nüîç {sentence}\\n ‚Üí Prediction: {label_map[label_id]} ({output['score']:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
